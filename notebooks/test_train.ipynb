{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "This is a script to debug why we cannot overfit in test_train test.\n",
    "\n",
    "Let first repeat the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(f'{os.getcwd()}/../../')\n",
    "\n",
    "from collections import OrderedDict\n",
    "from pprint import pprint\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "from housing_model.models.keras_model import ModelBuilder, HyperParams, TrainParams, bits_to_num, ArchitectureParams, \\\n",
    "    KerasModelTrainer, ModelParams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... ('Not JSON Serializable:', ModelBuilder(model_params=ModelParams(hyper_params=HyperParams(embedding_size=20), arc_params=ArchitectureParams(float_features={'date_end', 'map/lon', 'land/depth', 'land/front', 'map/lat'}, num_bits=32, price_feature_name='sold_price', bits_feature_name='bits')), debug_mode=None))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-07 23:43:45.439459: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2022-08-07 23:43:45.439476: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2022-08-07 23:43:45.439508: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... ('Not JSON Serializable:', ModelBuilder(model_params=ModelParams(hyper_params=HyperParams(embedding_size=20), arc_params=ArchitectureParams(float_features={'date_end', 'map/lon', 'land/depth', 'land/front', 'map/lat'}, num_bits=32, price_feature_name='sold_price', bits_feature_name='bits')), debug_mode=None))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1124.6387 - bits_loss: 1080.6350 - sold_price_loss: 44.0036"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-07 23:43:47.303075: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step - loss: 1124.6387 - bits_loss: 1080.6350 - sold_price_loss: 44.0036\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 1097.2863 - bits_loss: 1053.2826 - sold_price_loss: 44.0036"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-07 23:43:47.745631: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2022-08-07 23:43:47.745648: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step - loss: 1097.2863 - bits_loss: 1053.2826 - sold_price_loss: 44.0036\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1071.2065 - bits_loss: 1027.2764 - sold_price_loss: 43.9302\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1047.8735 - bits_loss: 1003.9434 - sold_price_loss: 43.9302\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1024.4436 - bits_loss: 980.5135 - sold_price_loss: 43.9302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-07 23:43:49.668421: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
      "2022-08-07 23:43:49.673549: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2022-08-07 23:43:49.680227: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: logs/scalars/20220807-234345/train/plugins/profile/2022_08_07_23_43_49\n",
      "\n",
      "2022-08-07 23:43:49.684961: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to logs/scalars/20220807-234345/train/plugins/profile/2022_08_07_23_43_49/Majids-MBP.trace.json.gz\n",
      "2022-08-07 23:43:49.693361: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: logs/scalars/20220807-234345/train/plugins/profile/2022_08_07_23_43_49\n",
      "\n",
      "2022-08-07 23:43:49.693736: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to logs/scalars/20220807-234345/train/plugins/profile/2022_08_07_23_43_49/Majids-MBP.memory_profile.json.gz\n",
      "2022-08-07 23:43:49.695357: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: logs/scalars/20220807-234345/train/plugins/profile/2022_08_07_23_43_49\n",
      "Dumped tool data for xplane.pb to logs/scalars/20220807-234345/train/plugins/profile/2022_08_07_23_43_49/Majids-MBP.xplane.pb\n",
      "Dumped tool data for overview_page.pb to logs/scalars/20220807-234345/train/plugins/profile/2022_08_07_23_43_49/Majids-MBP.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to logs/scalars/20220807-234345/train/plugins/profile/2022_08_07_23_43_49/Majids-MBP.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to logs/scalars/20220807-234345/train/plugins/profile/2022_08_07_23_43_49/Majids-MBP.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to logs/scalars/20220807-234345/train/plugins/profile/2022_08_07_23_43_49/Majids-MBP.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1001.0876 - bits_loss: 957.1574 - sold_price_loss: 43.9302\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 977.8271 - bits_loss: 933.8970 - sold_price_loss: 43.9302\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 954.6533 - bits_loss: 910.7231 - sold_price_loss: 43.9302\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 931.6281 - bits_loss: 887.6979 - sold_price_loss: 43.9302\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 908.7501 - bits_loss: 864.8199 - sold_price_loss: 43.9302\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 882.6213 - bits_loss: 842.2945 - sold_price_loss: 40.3267\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 850.1531 - bits_loss: 823.0867 - sold_price_loss: 27.0665\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 830.9651 - bits_loss: 803.8911 - sold_price_loss: 27.0742\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 811.9371 - bits_loss: 784.8548 - sold_price_loss: 27.0823\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 793.6731 - bits_loss: 766.5908 - sold_price_loss: 27.0824\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 776.1717 - bits_loss: 749.0894 - sold_price_loss: 27.0823\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 758.6816 - bits_loss: 731.6040 - sold_price_loss: 27.0776\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 741.3277 - bits_loss: 714.2598 - sold_price_loss: 27.0679\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 723.8696 - bits_loss: 696.8010 - sold_price_loss: 27.0685\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 706.5032 - bits_loss: 679.4260 - sold_price_loss: 27.0773\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 690.5443 - bits_loss: 663.4632 - sold_price_loss: 27.0811\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 674.4716 - bits_loss: 647.3905 - sold_price_loss: 27.0811\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 659.0240 - bits_loss: 631.9473 - sold_price_loss: 27.0767\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 643.8917 - bits_loss: 616.8230 - sold_price_loss: 27.0687\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 628.3975 - bits_loss: 601.3287 - sold_price_loss: 27.0689\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 613.3815 - bits_loss: 586.2742 - sold_price_loss: 27.1072\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 598.5787 - bits_loss: 571.4664 - sold_price_loss: 27.1123\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 583.5305 - bits_loss: 556.4166 - sold_price_loss: 27.1139\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 569.2902 - bits_loss: 542.1754 - sold_price_loss: 27.1148\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 555.0245 - bits_loss: 527.9158 - sold_price_loss: 27.1087\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 540.5217 - bits_loss: 513.4114 - sold_price_loss: 27.1103\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 526.6481 - bits_loss: 499.5357 - sold_price_loss: 27.1125\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 513.6739 - bits_loss: 486.5586 - sold_price_loss: 27.1153\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 501.1415 - bits_loss: 474.0276 - sold_price_loss: 27.1140\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 490.2980 - bits_loss: 463.1896 - sold_price_loss: 27.1084\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 479.0110 - bits_loss: 451.9049 - sold_price_loss: 27.1060\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 466.9931 - bits_loss: 439.8824 - sold_price_loss: 27.1107\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 454.9240 - bits_loss: 427.8412 - sold_price_loss: 27.0828\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 442.5355 - bits_loss: 415.4529 - sold_price_loss: 27.0826\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 430.3874 - bits_loss: 403.3081 - sold_price_loss: 27.0793\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 418.1444 - bits_loss: 391.0702 - sold_price_loss: 27.0742\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 405.5874 - bits_loss: 378.5150 - sold_price_loss: 27.0724\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 393.3802 - bits_loss: 366.3073 - sold_price_loss: 27.0729\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 381.2259 - bits_loss: 354.1509 - sold_price_loss: 27.0750\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 369.1142 - bits_loss: 342.0416 - sold_price_loss: 27.0726\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 356.9048 - bits_loss: 329.8344 - sold_price_loss: 27.0704\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 345.0158 - bits_loss: 317.9168 - sold_price_loss: 27.0990\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 332.8383 - bits_loss: 305.7358 - sold_price_loss: 27.1025\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 320.8682 - bits_loss: 293.7622 - sold_price_loss: 27.1060\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 310.3597 - bits_loss: 283.2544 - sold_price_loss: 27.1053\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 299.6471 - bits_loss: 272.5448 - sold_price_loss: 27.1023\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 290.1746 - bits_loss: 263.0734 - sold_price_loss: 27.1012\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 282.6739 - bits_loss: 255.5687 - sold_price_loss: 27.1052\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 274.6338 - bits_loss: 247.5569 - sold_price_loss: 27.0769\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 266.9903 - bits_loss: 239.9124 - sold_price_loss: 27.0778\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 259.2655 - bits_loss: 232.1848 - sold_price_loss: 27.0807\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 251.5960 - bits_loss: 224.5154 - sold_price_loss: 27.0806\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 243.7750 - bits_loss: 216.6944 - sold_price_loss: 27.0806\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 235.8079 - bits_loss: 208.7286 - sold_price_loss: 27.0793\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 227.3897 - bits_loss: 200.3117 - sold_price_loss: 27.0780\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 218.4634 - bits_loss: 191.3862 - sold_price_loss: 27.0772\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 209.0647 - bits_loss: 181.9869 - sold_price_loss: 27.0777\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 199.3498 - bits_loss: 172.2712 - sold_price_loss: 27.0785\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 190.5307 - bits_loss: 163.4208 - sold_price_loss: 27.1099\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 176.8202 - bits_loss: 155.9302 - sold_price_loss: 20.8900\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 162.8638 - bits_loss: 149.5748 - sold_price_loss: 13.2890\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 156.3052 - bits_loss: 143.0207 - sold_price_loss: 13.2845\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 149.8796 - bits_loss: 136.5817 - sold_price_loss: 13.2979\n",
      "Epoch 69/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 4ms/step - loss: 142.8800 - bits_loss: 129.5788 - sold_price_loss: 13.3013\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 136.0023 - bits_loss: 122.8228 - sold_price_loss: 13.1796\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 129.3759 - bits_loss: 116.1935 - sold_price_loss: 13.1824\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 124.0463 - bits_loss: 110.8309 - sold_price_loss: 13.2154\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 106.7371 - bits_loss: 106.3971 - sold_price_loss: 0.3401\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 103.5167 - bits_loss: 103.2243 - sold_price_loss: 0.2924\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 101.3903 - bits_loss: 100.9343 - sold_price_loss: 0.4560\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 101.3007 - bits_loss: 99.3138 - sold_price_loss: 1.9869\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 99.3762 - bits_loss: 97.4348 - sold_price_loss: 1.9414\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 96.7992 - bits_loss: 95.0467 - sold_price_loss: 1.7525\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 93.8626 - bits_loss: 92.1400 - sold_price_loss: 1.7226\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 89.0564 - bits_loss: 88.7669 - sold_price_loss: 0.2895\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 87.6693 - bits_loss: 87.2134 - sold_price_loss: 0.4559\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 86.9332 - bits_loss: 86.7349 - sold_price_loss: 0.1983\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 86.3968 - bits_loss: 86.1998 - sold_price_loss: 0.1970\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 86.0582 - bits_loss: 85.8611 - sold_price_loss: 0.1972\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 85.3346 - bits_loss: 85.1378 - sold_price_loss: 0.1968\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 83.7969 - bits_loss: 83.5994 - sold_price_loss: 0.1975\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 81.8375 - bits_loss: 81.6404 - sold_price_loss: 0.1971\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 79.7796 - bits_loss: 79.5579 - sold_price_loss: 0.2216\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 77.8979 - bits_loss: 77.7889 - sold_price_loss: 0.1090\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 76.8723 - bits_loss: 76.1908 - sold_price_loss: 0.6815\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 74.5790 - bits_loss: 74.3714 - sold_price_loss: 0.2076\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 73.0492 - bits_loss: 72.7310 - sold_price_loss: 0.3182\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 71.2367 - bits_loss: 70.9412 - sold_price_loss: 0.2955\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 69.2108 - bits_loss: 68.9161 - sold_price_loss: 0.2947\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 66.8929 - bits_loss: 66.6531 - sold_price_loss: 0.2398\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 64.9583 - bits_loss: 64.7602 - sold_price_loss: 0.1981\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 64.2072 - bits_loss: 63.5958 - sold_price_loss: 0.6113\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 63.0466 - bits_loss: 62.8387 - sold_price_loss: 0.2080\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 62.0236 - bits_loss: 61.8273 - sold_price_loss: 0.1963\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 60.8924 - bits_loss: 60.6790 - sold_price_loss: 0.2134\n",
      "sold_price:\n",
      "[870000.]\n",
      "\n",
      "bits:\n",
      "[[0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "expected: [870000.]\n",
      "res:\n",
      "1025367.6875\n",
      "0 - 0.0\n",
      "0 - 0.0\n",
      "0 - 0.0\n",
      "0 - 1.0\n",
      "1 - 0.27735546231269836\n",
      "1 - 5.692652295732903e-14\n",
      "1 - 0.9608322978019714\n",
      "0 - 1.8532671599603978e-13\n",
      "0 - 0.9999995231628418\n",
      "1 - 0.15994614362716675\n",
      "1 - 0.15800029039382935\n",
      "0 - 0.8772705793380737\n",
      "0 - 0.9999263882637024\n",
      "0 - 0.3789215087890625\n",
      "1 - 8.031354923332401e-07\n",
      "0 - 1.0\n",
      "1 - 0.9998418688774109\n",
      "0 - 1.0\n",
      "1 - 1.0\n",
      "1 - 1.0\n",
      "0 - 0.0\n",
      "0 - 1.048058706487609e-08\n",
      "0 - 0.0\n",
      "0 - 0.0\n",
      "0 - 0.0\n",
      "0 - 0.0\n",
      "0 - 0.0\n",
      "0 - 0.0\n",
      "0 - 0.0\n",
      "0 - 0.0\n",
      "0 - 0.0\n",
      "0 - 0.0\n"
     ]
    }
   ],
   "source": [
    "train_ds = tfds.load('tf_housing', split='train').take(6).cache()\n",
    "\n",
    "keras_model = KerasModelTrainer.build(ModelParams(\n",
    "        HyperParams(embedding_size=20),\n",
    "        ArchitectureParams.from_dataset(train_ds)\n",
    "    )\n",
    ")\n",
    "test_ds = keras_model.data_provider.setup_data(train_ds.take(1), batch_size=1)\n",
    "\n",
    "hist = keras_model.fit_model(\n",
    "    train_ds,\n",
    "    TrainParams(batch_size=6, epochs=100, learning_rate=1e-3)\n",
    ")\n",
    "output = keras_model.keras_model.predict(test_ds.map(lambda x, y: x))\n",
    "for idx, (_, expected) in enumerate(test_ds):\n",
    "    for k, v in expected.items():\n",
    "        print(f'{k}:\\n{v}\\n')\n",
    "    print(f'expected: {expected[\"sold_price\"]}\\nres:\\n{output[\"sold_price\"][idx]}')\n",
    "    for expected_bits, res_bit in zip(expected[\"bits\"][0], output[\"bits\"][idx]):\n",
    "        print(f'{expected_bits} - {res_bit}')\n",
    "    # print(f'bits:\\n{output[0][idx]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [1124.638671875,\n",
       "  1097.2862548828125,\n",
       "  1071.20654296875,\n",
       "  1047.87353515625,\n",
       "  1024.443603515625,\n",
       "  1001.0875854492188,\n",
       "  977.8271484375,\n",
       "  954.6532592773438,\n",
       "  931.6281127929688,\n",
       "  908.7500610351562,\n",
       "  882.6212768554688,\n",
       "  850.1531372070312,\n",
       "  830.9651489257812,\n",
       "  811.9370727539062,\n",
       "  793.673095703125,\n",
       "  776.1716918945312,\n",
       "  758.6815795898438,\n",
       "  741.3276977539062,\n",
       "  723.8695678710938,\n",
       "  706.5032348632812,\n",
       "  690.5442504882812,\n",
       "  674.4716186523438,\n",
       "  659.0240478515625,\n",
       "  643.8916625976562,\n",
       "  628.3975219726562,\n",
       "  613.3814697265625,\n",
       "  598.5786743164062,\n",
       "  583.530517578125,\n",
       "  569.2901611328125,\n",
       "  555.0244750976562,\n",
       "  540.5216674804688,\n",
       "  526.6481323242188,\n",
       "  513.6738891601562,\n",
       "  501.1415100097656,\n",
       "  490.2979736328125,\n",
       "  479.0109558105469,\n",
       "  466.9930725097656,\n",
       "  454.9239501953125,\n",
       "  442.5355224609375,\n",
       "  430.3873596191406,\n",
       "  418.1443786621094,\n",
       "  405.58740234375,\n",
       "  393.3802185058594,\n",
       "  381.2258605957031,\n",
       "  369.1141662597656,\n",
       "  356.90478515625,\n",
       "  345.0157775878906,\n",
       "  332.8383483886719,\n",
       "  320.8681945800781,\n",
       "  310.35968017578125,\n",
       "  299.6470947265625,\n",
       "  290.174560546875,\n",
       "  282.67388916015625,\n",
       "  274.6338195800781,\n",
       "  266.9902648925781,\n",
       "  259.2655029296875,\n",
       "  251.5960235595703,\n",
       "  243.7750244140625,\n",
       "  235.807861328125,\n",
       "  227.3897247314453,\n",
       "  218.46343994140625,\n",
       "  209.0646514892578,\n",
       "  199.3497772216797,\n",
       "  190.5306854248047,\n",
       "  176.8202362060547,\n",
       "  162.86380004882812,\n",
       "  156.30523681640625,\n",
       "  149.87957763671875,\n",
       "  142.88003540039062,\n",
       "  136.0023193359375,\n",
       "  129.37591552734375,\n",
       "  124.04634857177734,\n",
       "  106.73712921142578,\n",
       "  103.51665496826172,\n",
       "  101.3902587890625,\n",
       "  101.30065155029297,\n",
       "  99.3761978149414,\n",
       "  96.79920196533203,\n",
       "  93.86260986328125,\n",
       "  89.0564193725586,\n",
       "  87.66927337646484,\n",
       "  86.93316650390625,\n",
       "  86.3968276977539,\n",
       "  86.05823516845703,\n",
       "  85.33455657958984,\n",
       "  83.79692840576172,\n",
       "  81.8375244140625,\n",
       "  79.77957916259766,\n",
       "  77.89788818359375,\n",
       "  76.8722915649414,\n",
       "  74.5789566040039,\n",
       "  73.04923248291016,\n",
       "  71.23670196533203,\n",
       "  69.21078491210938,\n",
       "  66.89286804199219,\n",
       "  64.95829772949219,\n",
       "  64.2071533203125,\n",
       "  63.0466423034668,\n",
       "  62.02358627319336,\n",
       "  60.89240646362305],\n",
       " 'bits_loss': [1080.635009765625,\n",
       "  1053.2825927734375,\n",
       "  1027.2763671875,\n",
       "  1003.943359375,\n",
       "  980.5134887695312,\n",
       "  957.1574096679688,\n",
       "  933.89697265625,\n",
       "  910.7230834960938,\n",
       "  887.6979370117188,\n",
       "  864.8198852539062,\n",
       "  842.2944946289062,\n",
       "  823.0867309570312,\n",
       "  803.8910522460938,\n",
       "  784.8547973632812,\n",
       "  766.5907592773438,\n",
       "  749.0894165039062,\n",
       "  731.60400390625,\n",
       "  714.259765625,\n",
       "  696.801025390625,\n",
       "  679.4259643554688,\n",
       "  663.4631958007812,\n",
       "  647.3905029296875,\n",
       "  631.9473266601562,\n",
       "  616.822998046875,\n",
       "  601.3286743164062,\n",
       "  586.2742309570312,\n",
       "  571.4663696289062,\n",
       "  556.4165649414062,\n",
       "  542.1753540039062,\n",
       "  527.9158325195312,\n",
       "  513.411376953125,\n",
       "  499.5356750488281,\n",
       "  486.55859375,\n",
       "  474.027587890625,\n",
       "  463.1896057128906,\n",
       "  451.9049377441406,\n",
       "  439.8824157714844,\n",
       "  427.8411865234375,\n",
       "  415.452880859375,\n",
       "  403.3080749511719,\n",
       "  391.0701599121094,\n",
       "  378.5149841308594,\n",
       "  366.3072509765625,\n",
       "  354.15087890625,\n",
       "  342.0415954589844,\n",
       "  329.8343811035156,\n",
       "  317.9167785644531,\n",
       "  305.7358093261719,\n",
       "  293.7621765136719,\n",
       "  283.2543640136719,\n",
       "  272.5447692871094,\n",
       "  263.0733947753906,\n",
       "  255.5686798095703,\n",
       "  247.556884765625,\n",
       "  239.91241455078125,\n",
       "  232.1847686767578,\n",
       "  224.51539611816406,\n",
       "  216.69439697265625,\n",
       "  208.7285919189453,\n",
       "  200.31170654296875,\n",
       "  191.38621520996094,\n",
       "  181.9869384765625,\n",
       "  172.27122497558594,\n",
       "  163.42079162597656,\n",
       "  155.93017578125,\n",
       "  149.57484436035156,\n",
       "  143.0207061767578,\n",
       "  136.58168029785156,\n",
       "  129.57875061035156,\n",
       "  122.82275390625,\n",
       "  116.19354248046875,\n",
       "  110.83092498779297,\n",
       "  106.39705657958984,\n",
       "  103.2242660522461,\n",
       "  100.93428802490234,\n",
       "  99.31378173828125,\n",
       "  97.43480682373047,\n",
       "  95.0467300415039,\n",
       "  92.13997650146484,\n",
       "  88.76691436767578,\n",
       "  87.21338653564453,\n",
       "  86.73487091064453,\n",
       "  86.19983673095703,\n",
       "  85.86107635498047,\n",
       "  85.13775634765625,\n",
       "  83.59941864013672,\n",
       "  81.64044952392578,\n",
       "  79.55792999267578,\n",
       "  77.78885650634766,\n",
       "  76.1907958984375,\n",
       "  74.3713607788086,\n",
       "  72.73101806640625,\n",
       "  70.9411849975586,\n",
       "  68.91609954833984,\n",
       "  66.65308380126953,\n",
       "  64.7601547241211,\n",
       "  63.59584426879883,\n",
       "  62.83865737915039,\n",
       "  61.8272819519043,\n",
       "  60.6789665222168],\n",
       " 'sold_price_loss': [44.00364303588867,\n",
       "  44.00364303588867,\n",
       "  43.93017578125,\n",
       "  43.93017578125,\n",
       "  43.93017578125,\n",
       "  43.93017578125,\n",
       "  43.93017578125,\n",
       "  43.93017578125,\n",
       "  43.93017578125,\n",
       "  43.93017578125,\n",
       "  40.32673263549805,\n",
       "  27.06648063659668,\n",
       "  27.07417869567871,\n",
       "  27.082277297973633,\n",
       "  27.082365036010742,\n",
       "  27.082321166992188,\n",
       "  27.077573776245117,\n",
       "  27.067914962768555,\n",
       "  27.06853675842285,\n",
       "  27.077293395996094,\n",
       "  27.081132888793945,\n",
       "  27.081092834472656,\n",
       "  27.076692581176758,\n",
       "  27.068689346313477,\n",
       "  27.068857192993164,\n",
       "  27.107229232788086,\n",
       "  27.11229705810547,\n",
       "  27.113924026489258,\n",
       "  27.114831924438477,\n",
       "  27.108652114868164,\n",
       "  27.11027717590332,\n",
       "  27.112459182739258,\n",
       "  27.11528968811035,\n",
       "  27.11396026611328,\n",
       "  27.10837745666504,\n",
       "  27.10602569580078,\n",
       "  27.11065673828125,\n",
       "  27.0827693939209,\n",
       "  27.0826358795166,\n",
       "  27.079320907592773,\n",
       "  27.07420539855957,\n",
       "  27.07242774963379,\n",
       "  27.072935104370117,\n",
       "  27.074995040893555,\n",
       "  27.07256507873535,\n",
       "  27.070405960083008,\n",
       "  27.099044799804688,\n",
       "  27.1025390625,\n",
       "  27.10601806640625,\n",
       "  27.10532569885254,\n",
       "  27.102340698242188,\n",
       "  27.101150512695312,\n",
       "  27.105224609375,\n",
       "  27.076940536499023,\n",
       "  27.077842712402344,\n",
       "  27.08072853088379,\n",
       "  27.08063507080078,\n",
       "  27.08062171936035,\n",
       "  27.07926368713379,\n",
       "  27.07801628112793,\n",
       "  27.077219009399414,\n",
       "  27.077714920043945,\n",
       "  27.078535079956055,\n",
       "  27.10987091064453,\n",
       "  20.890045166015625,\n",
       "  13.288960456848145,\n",
       "  13.284534454345703,\n",
       "  13.297898292541504,\n",
       "  13.301287651062012,\n",
       "  13.179564476013184,\n",
       "  13.182379722595215,\n",
       "  13.21541690826416,\n",
       "  0.34007057547569275,\n",
       "  0.29239001870155334,\n",
       "  0.4559743106365204,\n",
       "  1.9868675470352173,\n",
       "  1.9413985013961792,\n",
       "  1.7524738311767578,\n",
       "  1.7226330041885376,\n",
       "  0.28950804471969604,\n",
       "  0.4558771550655365,\n",
       "  0.19829298555850983,\n",
       "  0.19700179994106293,\n",
       "  0.19715513288974762,\n",
       "  0.196793794631958,\n",
       "  0.19750775396823883,\n",
       "  0.19707174599170685,\n",
       "  0.22164678573608398,\n",
       "  0.10903220623731613,\n",
       "  0.6814946532249451,\n",
       "  0.2075933814048767,\n",
       "  0.31821581721305847,\n",
       "  0.2955166697502136,\n",
       "  0.29468512535095215,\n",
       "  0.2397807091474533,\n",
       "  0.19814543426036835,\n",
       "  0.6113119125366211,\n",
       "  0.20798595249652863,\n",
       "  0.1963033676147461,\n",
       "  0.21343912184238434]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss does not go to zero. Lets first check the gradiant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_grads():\n",
    "    x, y = iter(test_ds).next()\n",
    "    loss = tf.keras.losses.BinaryCrossentropy()\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(y['bits'], keras_model._model(x)['bits'])\n",
    "    print(f'The loss value is {loss_value}\\n')\n",
    "\n",
    "    grads = tape.gradient(loss_value, keras_model._model.trainable_weights)\n",
    "    for weigths, grad in zip(\n",
    "        reversed(keras_model._model.trainable_weights), \n",
    "        reversed(grads)\n",
    "    ):\n",
    "        print(weigths.name)\n",
    "        print(tf.norm(grad).numpy())\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_grads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model.train(TrainParams(batch_size=3, epochs=500, learning_rate=1e-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_grads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model.train(TrainParams(batch_size=3, epochs=500, learning_rate=1e-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_grads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems even when we have gradiant, the loss value does not go down!\n",
    "The value of gradiant is not too large (less than 1.0), so the learning rate should be fine. \n",
    "1. If no, what if we apply gradiant, does it overfit?\n",
    "1. Do we have contracting example in the trainign data set?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "\n",
    "for i in range(10):\n",
    "    for x, y in test_ds:\n",
    "        loss_fn = lambda: loss(keras_model._model(x)['bits'], tf.cast(y['bits'], tf.float32))\n",
    "        opt.minimize(loss_fn, keras_model._model.trainable_weights)\n",
    "\n",
    "print_grads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This suggest either the training data sets are inconsistant or feature generator has some issue. If so, we will reach to an imperfect model if we train again on the same data sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model.train(TrainParams(batch_size=3, epochs=500, learning_rate=1e-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_grads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even the loss value has been increased, it cannot show that there is inconsistency between training examples. Let check every pair of examples and see if there is a pair that has more deficult to overfit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_loss = 0\n",
    "worst_paris = None\n",
    "train_examples = keras_model.setup_data(train_ds, batch_size=1)\n",
    "for x0, y0 in tqdm.tqdm(train_examples):\n",
    "    for x1, y1 in train_examples:        \n",
    "        two_x = tf.data.Dataset.from_tensor_slices(x0).concatenate(\n",
    "            tf.data.Dataset.from_tensor_slices(x1)\n",
    "        )\n",
    "        two_y = tf.data.Dataset.from_tensor_slices(y0).concatenate(\n",
    "            tf.data.Dataset.from_tensor_slices(y1)\n",
    "        )\n",
    "        \n",
    "        two_ex = tf.data.Dataset.zip((two_x, two_y)).batch(batch_size=2)\n",
    "        history = keras_model._model.fit(two_ex, epochs=1000, verbose=0)\n",
    "        loss_val = history.history['loss'][-1]\n",
    "        if loss_val > max_loss:\n",
    "            max_loss = loss_val  \n",
    "            worst_paris = two_ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in two_ex:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in train_dsxamples:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
